{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e69e649b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10b2bcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39f0adf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, {'color': 'gray', 'size': 450})\n",
      "(1, {'color': 'yellow', 'size': 700})\n",
      "(2, {'color': 'red', 'size': 250})\n",
      "(3, {'color': 'pink', 'size': 500})\n"
     ]
    }
   ],
   "source": [
    "H = nx.DiGraph()\n",
    "H.add_nodes_from([\n",
    "  (0, {\"color\": \"gray\", \"size\": 450}),\n",
    "  (1, {\"color\": \"yellow\", \"size\": 700}),\n",
    "  (2, {\"color\": \"red\", \"size\": 250}),\n",
    "  (3, {\"color\": \"pink\", \"size\": 500})\n",
    "])\n",
    "for node in H.nodes(data=True):\n",
    "  print(node)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36a28cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 2), (2, 0), (2, 3), (3, 2)]\n"
     ]
    }
   ],
   "source": [
    "H.add_edges_from([\n",
    "  (0, 1),\n",
    "  (1, 2),\n",
    "  (2, 0),\n",
    "  (2, 3),\n",
    "  (3, 2)\n",
    "])\n",
    "print(H.edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dba49e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directed graph: True\n",
      "Number of nodes: 4\n",
      "Number of edges: 5\n"
     ]
    }
   ],
   "source": [
    "def print_graph_info(graph):\n",
    "  print(\"Directed graph:\", graph.is_directed())\n",
    "  print(\"Number of nodes:\", graph.number_of_nodes())\n",
    "  print(\"Number of edges:\", graph.number_of_edges())\n",
    "print_graph_info(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "782acc2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key text.latex.unicode in file /dartfs-hpc/rc/home/d/f0036rd/.conda/envs/SHAPenv/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 112 ('text.latex.unicode : False # use \"ucs\" and \"inputenc\" LaTeX packages for handling')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key text.latex.preview in file /dartfs-hpc/rc/home/d/f0036rd/.conda/envs/SHAPenv/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 125 ('text.latex.preview : False')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key mathtext.fallback_to_cm in file /dartfs-hpc/rc/home/d/f0036rd/.conda/envs/SHAPenv/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 157 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key savefig.jpeg_quality in file /dartfs-hpc/rc/home/d/f0036rd/.conda/envs/SHAPenv/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 420 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key savefig.frameon in file /dartfs-hpc/rc/home/d/f0036rd/.conda/envs/SHAPenv/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 423 ('savefig.frameon : True')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key pgf.debug in file /dartfs-hpc/rc/home/d/f0036rd/.conda/envs/SHAPenv/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 444 ('pgf.debug           : False')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key verbose.level in file /dartfs-hpc/rc/home/d/f0036rd/.conda/envs/SHAPenv/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 475 ('verbose.level  : silent      # one of silent, helpful, debug, debug-annoying')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key verbose.fileo in file /dartfs-hpc/rc/home/d/f0036rd/.conda/envs/SHAPenv/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 476 ('verbose.fileo  : sys.stdout  # a log filename, sys.stdout or sys.stderr')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key keymap.all_axes in file /dartfs-hpc/rc/home/d/f0036rd/.conda/envs/SHAPenv/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 493 ('keymap.all_axes : a                 # enable all axes')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key animation.avconv_path in file /dartfs-hpc/rc/home/d/f0036rd/.conda/envs/SHAPenv/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 504 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key animation.avconv_args in file /dartfs-hpc/rc/home/d/f0036rd/.conda/envs/SHAPenv/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 506 ('animation.avconv_args:            # Additional arguments to pass to avconv')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "node_colors = nx.get_node_attributes(H, \"color\").values()\n",
    "colors = list(node_colors)\n",
    "\n",
    "node_sizes = nx.get_node_attributes(H, \"size\").values()\n",
    "sizes = list(node_sizes)\n",
    "\n",
    "nx.draw(H, with_labels=True, node_color=colors, node_size=sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcb5c9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "359baae6",
   "metadata": {},
   "source": [
    "# Tooling around with the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa153c34",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/scratch/ipykernel_28751/4265195184.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/SHAPenv/lib/python3.7/site-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[0;31m# Shared memory manager needs to know the exact location of manager executable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m \u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initExtension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmanager_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mmanager_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/SHAPenv/lib/python3.7/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0m_queued_calls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m \u001b[0m_lazy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_check_capability\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0m_lazy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_check_cubins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/SHAPenv/lib/python3.7/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_call\u001b[0;34m(callable)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;31m# Don't store the actual traceback to avoid memory cycle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0m_queued_calls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0m_lazy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_check_capability\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/SHAPenv/lib/python3.7/traceback.py\u001b[0m in \u001b[0;36mformat_stack\u001b[0;34m(f, limit)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mformat_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/SHAPenv/lib/python3.7/traceback.py\u001b[0m in \u001b[0;36mextract_stack\u001b[0;34m(f, limit)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m     \u001b[0mstack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStackSummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwalk_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m     \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/SHAPenv/lib/python3.7/traceback.py\u001b[0m in \u001b[0;36mextract\u001b[0;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlookup_lines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/SHAPenv/lib/python3.7/traceback.py\u001b[0m in \u001b[0;36mline\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_line\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinecache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlineno\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/SHAPenv/lib/python3.7/linecache.py\u001b[0m in \u001b[0;36mgetline\u001b[0;34m(filename, lineno, module_globals)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlineno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_globals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_globals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mlineno\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlineno\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/SHAPenv/lib/python3.7/linecache.py\u001b[0m in \u001b[0;36mgetlines\u001b[0;34m(filename, module_globals)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdatecache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_globals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mMemoryError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mclearcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/SHAPenv/lib/python3.7/linecache.py\u001b[0m in \u001b[0;36mupdatecache\u001b[0;34m(filename, module_globals)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfullname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m             \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/SHAPenv/lib/python3.7/tokenize.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m         \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetect_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m         \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextIOWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline_buffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/SHAPenv/lib/python3.7/tokenize.py\u001b[0m in \u001b[0;36mdetect_encoding\u001b[0;34m(readline)\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m     \u001b[0mfirst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_or_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfirst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBOM_UTF8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0mbom_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/SHAPenv/lib/python3.7/tokenize.py\u001b[0m in \u001b[0;36mread_or_stop\u001b[0;34m()\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread_or_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34mb''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "787af839",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_C' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/scratch/ipykernel_28751/3878246058.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#print(os.environ)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKarateClub\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/SHAPenv/lib/python3.7/site-packages/torch_geometric/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mimportlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimport_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/SHAPenv/lib/python3.7/site-packages/torch_geometric/data/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mhetero_data\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHeteroData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtemporal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTemporalData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/SHAPenv/lib/python3.7/site-packages/torch_geometric/data/data.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_sparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparseTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/SHAPenv/lib/python3.7/site-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m __all__ += [name for name in dir(_C)\n\u001b[0m\u001b[1;32m    230\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             not name.endswith('Base')]\n",
      "\u001b[0;31mNameError\u001b[0m: name '_C' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#print(os.environ)\n",
    "import torch_geometric.datasets\n",
    "from torch_geometric.datasets import KarateClub\n",
    "\n",
    "dataset = KarateClub()\n",
    "print(\"Dataset:\", dataset)\n",
    "print(\"# Graphs:\", len(dataset))\n",
    "print(\"# Features:\", dataset.num_features)\n",
    "print(\"# Classes:\", dataset.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe2f20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install cudatoolkit=7.5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9913fc",
   "metadata": {},
   "source": [
    "# Tooling Around Part 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c99fc440",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dartfs-hpc/rc/home/d/f0036rd/.conda/envs/SHAPenv/lib/python3.7/site-packages/requests/__init__.py:104: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (5.0.0)/charset_normalizer (2.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n",
      "2022-07-08 09:33:54.251709: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-07-08 09:33:54.252891: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-08 09:33:54.258537: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "\n",
    "class ShapNN(object):\n",
    "    \n",
    "    def __init__(self, mode, hidden_units=[100], learning_rate=0.001, \n",
    "                 dropout = 0., activation=None, initializer=None,\n",
    "                 weight_decay=0.0001, optimizer='adam', batch_size=128,\n",
    "                 warm_start=False, max_epochs=100, validation_fraction=0.1,\n",
    "                 early_stopping=0, address=None, test_batch_size=1000,\n",
    "                 random_seed=666):\n",
    "        \n",
    "        self.mode = mode\n",
    "        self.batch_size = batch_size\n",
    "        self.test_batch_size = test_batch_size\n",
    "        self.hidden_units = hidden_units\n",
    "        self.initializer = initializer\n",
    "        self.activation = activation\n",
    "        self.dropout = dropout\n",
    "        self.weight_decay = weight_decay\n",
    "        self.optimizer = optimizer\n",
    "        self.learning_rate = learning_rate\n",
    "        self.warm_start = warm_start\n",
    "        self.max_epochs = max_epochs\n",
    "        self.early_stopping = early_stopping\n",
    "        self.validation_fraction = validation_fraction\n",
    "        self.address = address\n",
    "        self._extra_train_ops = []\n",
    "        self.random_seed = random_seed\n",
    "        self.is_built = False\n",
    "\n",
    "    def prediction_cost(self, X_test, y_test, batch_size=None):\n",
    "        \n",
    "        if batch_size is None:\n",
    "            batch_size = self.test_batch_size\n",
    "        assert len(set(y_test)) == self.num_classes, 'Number of classes does not match!'\n",
    "        with self.graph.as_default():\n",
    "            losses = []\n",
    "            idxs = np.arange(len(X_test))            \n",
    "            batches = [idxs[k * batch_size: (k+1) * batch_size] \n",
    "                       for k in range(int(np.ceil(len(idxs)/batch_size)))]\n",
    "            for batch in batches:\n",
    "                losses.append(self.sess.run(self.prediction_loss, {self.input_ph:X_test[batch],\n",
    "                                                                   self.labels:y_test[batch]}))\n",
    "            return np.mean(losses)     \n",
    "        \n",
    "    def score(self, X_test, y_test, batch_size=None):\n",
    "        \n",
    "        if batch_size is None:\n",
    "            batch_size = self.test_batch_size\n",
    "        assert len(set(y_test)) == self.num_classes, 'Number of classes does not match!'\n",
    "        with self.graph.as_default():\n",
    "            scores = []\n",
    "            idxs = np.arange(len(X_test))     \n",
    "            batches = [idxs[k * batch_size: (k+1) * batch_size] \n",
    "                       for k in range(int(np.ceil(len(idxs)/batch_size)))]\n",
    "            for batch in batches:\n",
    "                scores.append(self.sess.run(self.prediction_score, {self.input_ph:X_test[batch],\n",
    "                                                                   self.labels:y_test[batch]}))\n",
    "            return np.mean(scores)\n",
    "        \n",
    "    def predict_proba(self, X_test, batch_size=None):\n",
    "        \n",
    "        if batch_size is None:\n",
    "            batch_size = self.test_batch_size\n",
    "        with self.graph.as_default():\n",
    "            probs = []\n",
    "            idxs = np.arange(len(X_test))     \n",
    "            batches = [idxs[k * batch_size: (k+1) * batch_size] \n",
    "                       for k in range(int(np.ceil(len(idxs)/batch_size)))]\n",
    "            for batch in batches:\n",
    "                probs.append(self.sess.run(self.probs, {self.input_ph:X_test[batch]}))\n",
    "            return np.concatenate(probs, axis=0)    \n",
    "        \n",
    "    def predict_log_proba(self, X_test, batch_size=None):\n",
    "        \n",
    "        if batch_size is None:\n",
    "            batch_size = self.test_batch_size\n",
    "        with self.graph.as_default():\n",
    "            probs = []\n",
    "            idxs = np.arange(len(X_test))            \n",
    "            batches = [idxs[k * batch_size: (k+1) * batch_size] \n",
    "                       for k in range(int(np.ceil(len(idxs)/batch_size)))]\n",
    "            for batch in batches:\n",
    "                probs.append(self.sess.run(self.probs, {self.input_ph:X_test[batch]}))\n",
    "            return np.log(np.clip(np.concatenate(probs), 1e-12, None))   \n",
    "        \n",
    "    def cost(self, X_test, y_test, batch_size=None):\n",
    "        \n",
    "        if batch_size is None:\n",
    "            batch_size = self.batch_size\n",
    "        with self.graph.as_default():\n",
    "            losss = []\n",
    "            idxs = np.arange(len(X_test))            \n",
    "            batches = [idxs[k * batch_size: (k+1) * batch_size] \n",
    "                       for k in range(int(np.ceil(len(idxs)/batch_size)))]\n",
    "            for batch in batches:\n",
    "                losss.append(self.sess.run(self.prediction_loss, {self.input_ph:X_test[batch],\n",
    "                                                                   self.labels:y_test[batch]}))\n",
    "            return np.mean(losss)\n",
    "    \n",
    "    def predict(self, X_test, batch_size=None):\n",
    "        \n",
    "        if batch_size is None:\n",
    "            batch_size = self.batch_size\n",
    "        with self.graph.as_default():\n",
    "            predictions = []\n",
    "            idxs = np.arange(len(X_test))\n",
    "            batches = [idxs[k * batch_size: (k+1) * batch_size] \n",
    "                       for k in range(int(np.ceil(len(idxs)/batch_size)))]\n",
    "            for batch in batches:\n",
    "                predictions.append(self.sess.run(self.predictions, {self.input_ph:X_test[batch]}))\n",
    "            return np.concatenate(predictions)\n",
    "        \n",
    "    def fit(self, X, y, X_val=None, y_val=None, sources=None, max_epochs=None,\n",
    "            batch_size=None, save=False, load=False, sample_weight=None,\n",
    "            metric='accuracy'):\n",
    "        \n",
    "        self.num_classes = len(set(y))\n",
    "        self.metric = metric\n",
    "        if max_epochs is None:\n",
    "            max_epochs = self.max_epochs\n",
    "        if batch_size is None:\n",
    "            batch_size = self.batch_size\n",
    "        if not self.is_built:\n",
    "            self.graph = tf.Graph() \n",
    "            with self.graph.as_default():\n",
    "                config = tf.ConfigProto()\n",
    "                config.gpu_options.allow_growth=True\n",
    "                self.sess = tf.Session(config=config)\n",
    "        with self.graph.as_default():\n",
    "            tf.set_random_seed(self.random_seed)\n",
    "            try:\n",
    "                self.global_step = tf.train.create_global_step()\n",
    "            except ValueError:\n",
    "                self.global_step = tf.train.get_global_step()\n",
    "            if not self.is_built:\n",
    "                self._build_model(X, y)\n",
    "                self.saver = tf.train.Saver()\n",
    "            self._initialize()\n",
    "            if len(X):\n",
    "                if X_val is None and self.validation_fraction * len(X) > 2:\n",
    "                    X_train, X_val, y_train, y_val, sample_weight, _ = train_test_split(\n",
    "                        X, y, sample_weight, test_size=self.validation_fraction)\n",
    "                else:\n",
    "                    X_train, y_train = X, y\n",
    "                self._train_model(X_train, y_train, X_val=X_val, y_val=y_val,\n",
    "                                  max_epochs=max_epochs, batch_size=batch_size,\n",
    "                                  sources=sources, sample_weight=sample_weight)\n",
    "                if save and self.address is not None:\n",
    "                    self.saver.save(self.sess, self.address)\n",
    "            \n",
    "    def _train_model(self, X, y, X_val, y_val, max_epochs, batch_size, \n",
    "                     sources=None, sample_weight=None):\n",
    "        \n",
    "        \n",
    "        assert len(X)==len(y), 'Input and labels not the same size'\n",
    "        self.history = {'metrics':[], 'idxs':[]}\n",
    "        stop_counter = 0\n",
    "        best_performance = None\n",
    "        for epoch in range(max_epochs):\n",
    "            vals_metrics, idxs = self._one_epoch(\n",
    "                X, y, X_val, y_val, batch_size, sources=sources, sample_weight=sample_weight)\n",
    "            self.history['idxs'].append(idxs)\n",
    "            self.history['metrics'].append(vals_metrics)\n",
    "            if self.early_stopping and X_val is not None:\n",
    "                current_performance = np.mean(val_acc)\n",
    "                if best_performance is None:\n",
    "                    best_performance = current_performance\n",
    "                if current_performance > best_performance:\n",
    "                    best_performance = current_performance\n",
    "                    stop_counter = 0\n",
    "                else:\n",
    "                    stop_counter += 1\n",
    "                    if stop_counter > self.early_stopping:\n",
    "                        break\n",
    "        \n",
    "    def _one_epoch(self, X, y, X_val, y_val, batch_size, sources=None, sample_weight=None):\n",
    "        \n",
    "        vals = []\n",
    "        if sources is None:\n",
    "            if sample_weight is None:\n",
    "                idxs = np.random.permutation(len(X))\n",
    "            else:\n",
    "                idxs = np.random.choice(len(X), len(X), p=sample_weight/np.sum(sample_weight))    \n",
    "            batches = [idxs[k*batch_size:(k+1) * batch_size]\n",
    "                       for k in range(int(np.ceil(len(idxs)/batch_size)))]\n",
    "            idxs = batches\n",
    "        else:\n",
    "            idxs = np.random.permutation(len(sources.keys()))\n",
    "            batches = [sources[i] for i in idxs]\n",
    "        for batch_counter, batch in enumerate(batches):\n",
    "            self.sess.run(self.train_op, \n",
    "                          {self.input_ph:X[batch], self.labels:y[batch],\n",
    "                           self.dropout_ph:self.dropout})\n",
    "            if X_val is not None:\n",
    "                if self.metric=='accuracy':\n",
    "                    vals.append(self.score(X_val, y_val))\n",
    "                elif self.metric=='f1':\n",
    "                    vals.append(f1_score(y_val, self.predict(X_val)))\n",
    "                elif self.metric=='auc':\n",
    "                    vals.append(roc_auc_score(y_val, self.predict_proba(X_val)[:,1]))\n",
    "                elif self.metric=='xe':\n",
    "                    vals.append(-self.prediction_cost(X_val, y_val))\n",
    "        return np.array(vals), np.array(idxs)\n",
    "    \n",
    "    def _initialize(self):\n",
    "        \n",
    "        uninitialized_vars = []\n",
    "        if self.warm_start:\n",
    "            for var in tf.global_variables():\n",
    "                try:\n",
    "                    self.sess.run(var)\n",
    "                except tf.errors.FailedPreconditionError:\n",
    "                    uninitialized_vars.append(var)\n",
    "        else:\n",
    "            uninitialized_vars = tf.global_variables()\n",
    "        self.sess.run(tf.initializers.variables(uninitialized_vars))\n",
    "        \n",
    "    def _build_model(self, X, y):\n",
    "        \n",
    "        self.num_classes = len(set(y))\n",
    "        if self.initializer is None:\n",
    "            initializer = tf.initializers.variance_scaling(distribution='uniform')\n",
    "        if self.activation is None:\n",
    "            activation = lambda x: tf.nn.relu(x)\n",
    "        self.input_ph = tf.placeholder(dtype=tf.float32, shape=(None,) + X.shape[1:], name='input')\n",
    "        self.dropout_ph = tf.placeholder_with_default(\n",
    "            tf.constant(0., dtype=tf.float32), shape=(), name='dropout')\n",
    "        if self.mode=='regression':\n",
    "            self.labels = tf.placeholder(dtype=tf.float32, shape=(None, ), name='label')\n",
    "        else:\n",
    "            self.labels = tf.placeholder(dtype=tf.int32, shape=(None, ), name='label')\n",
    "        x = tf.reshape(self.input_ph, shape=(-1, np.prod(X.shape[1:])))\n",
    "        for layer, hidden_unit in enumerate(self.hidden_units):\n",
    "            with tf.variable_scope('dense_{}'.format(layer)):\n",
    "                x = self._dense(x, hidden_unit, dropout=self.dropout_ph, \n",
    "                           initializer=self.initializer, activation=activation)\n",
    "        with tf.variable_scope('final'):\n",
    "            self.prelogits = x\n",
    "            self._final_layer(self.prelogits, self.num_classes, self.mode)\n",
    "        self._build_train_op()\n",
    "        \n",
    "    def _build_train_op(self):\n",
    "        \n",
    "        \"\"\"Build taining specific ops for the graph.\"\"\"\n",
    "        learning_rate = tf.constant(self.learning_rate, tf.float32) ##fixit\n",
    "        trainable_variables = tf.trainable_variables()\n",
    "        grads = tf.gradients(self.loss, trainable_variables)\n",
    "        self.grad_flat = tf.concat([tf.reshape(grad, (-1, 1)) for grad in grads], axis=0)\n",
    "        if self.optimizer == 'sgd':\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        elif self.optimizer == 'mom':\n",
    "            optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9)\n",
    "        elif self.optimizer == 'adam':\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        apply_op = optimizer.apply_gradients(\n",
    "            zip(grads, trainable_variables),\n",
    "            global_step=self.global_step, name='train_step')\n",
    "        train_ops = [apply_op] + self._extra_train_ops + tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        previous_ops = [tf.group(*train_ops)]\n",
    "        with tf.control_dependencies(previous_ops):\n",
    "            self.train_op = tf.no_op(name='train')   \n",
    "        self.is_built = True\n",
    "    \n",
    "    def _final_layer(self, x, num_classes, mode):\n",
    "        \n",
    "        if mode=='regression':\n",
    "            self.logits = self._dense(x, 1, dropout=self.dropout_ph)\n",
    "            self.predictions = tf.reduce_sum(self.logits, axis=-1)\n",
    "            regression_loss = tf.nn.l2_loss(self.predictions - self.labels) ##FIXIT\n",
    "            self.prediction_loss = tf.reduce_mean(regression_loss, name='l2')\n",
    "            residuals = self.predictions - self.labels\n",
    "            var_predicted = tf.reduce_mean(residuals**2) - tf.reduce_mean(residuals)**2\n",
    "            var_labels = tf.reduce_mean(self.labels**2) - tf.reduce_mean(self.labels)**2\n",
    "            self.prediction_score = 1 - var_predicted/(var_labels + 1e-12)\n",
    "        else:\n",
    "            self.logits = self._dense(x, num_classes, dropout=self.dropout_ph)\n",
    "            self.probs = tf.nn.softmax(self.logits)\n",
    "            xent_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits=self.logits, labels=tf.cast(self.labels, tf.int32))\n",
    "            self.prediction_loss = tf.reduce_mean(xent_loss, name='xent')\n",
    "            self.predictions = tf.argmax(self.probs, axis=-1, output_type=tf.int32)\n",
    "            correct_predictions = tf.equal(self.predictions, self.labels)\n",
    "            self.prediction_score = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
    "        self.loss = self.prediction_loss + self._reg_loss()\n",
    "                \n",
    "    def _dense(self, x, out_dim, dropout=tf.constant(0.), initializer=None, activation=None):\n",
    "        \n",
    "        if initializer is None:\n",
    "            initializer = tf.initializers.variance_scaling(distribution='uniform')\n",
    "        w = tf.get_variable('DW', [x.get_shape()[1], out_dim], initializer=initializer)\n",
    "        b = tf.get_variable('Db', [out_dim], initializer=tf.constant_initializer())\n",
    "        x = tf.nn.dropout(x, 1. - dropout)\n",
    "        if activation:\n",
    "            x = activation(x)\n",
    "        return tf.nn.xw_plus_b(x, w, b)\n",
    "    \n",
    "    def _reg_loss(self, order=2):\n",
    "        \"\"\"Regularization loss for weight decay.\"\"\"\n",
    "        losss = []\n",
    "        for var in tf.trainable_variables():\n",
    "            if var.op.name.find(r'DW') > 0 or var.op.name.find(r'CW') > 0: ##FIXIT\n",
    "                if order==2:\n",
    "                    losss.append(tf.nn.l2_loss(var))\n",
    "                elif order==1:\n",
    "                    losss.append(tf.abs(var))\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid regularization order!\")\n",
    "        return tf.multiply(self.weight_decay, tf.add_n(losss))\n",
    "\n",
    "\n",
    "class CShapNN(ShapNN):\n",
    "    \n",
    "    def __init__(self, mode, hidden_units=[100], kernel_sizes=[], \n",
    "                 strides=None, channels=[], learning_rate=0.001, \n",
    "                 dropout = 0., activation=None, initializer=None, global_averaging=False,\n",
    "                weight_decay=0.0001, optimizer='adam', batch_size=128, \n",
    "                warm_start=False, max_epochs=100, validation_fraction=0.1,\n",
    "                early_stopping=0, address=None, test_batch_size=1000, random_seed=666):\n",
    "        \n",
    "        self.mode = mode\n",
    "        self.test_batch_size = test_batch_size\n",
    "        self.kernels = []#FIXIT\n",
    "        self.kernel_sizes = kernel_sizes\n",
    "        self.channels = channels\n",
    "        self.global_averaging = global_averaging\n",
    "        assert len(channels)==len(kernel_sizes), 'Invalid channels or kernel_sizes'\n",
    "        if strides is None:\n",
    "            self.strides = [1] * len(kernel_sizes)\n",
    "        else:\n",
    "            self.strides = strides\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_units = hidden_units\n",
    "        self.initializer = initializer\n",
    "        self.activation = activation\n",
    "        self.dropout = dropout\n",
    "        self.weight_decay = weight_decay\n",
    "        self.optimizer = optimizer\n",
    "        self.learning_rate = learning_rate\n",
    "        self.warm_start = warm_start\n",
    "        self.max_epochs = max_epochs\n",
    "        self.early_stopping = early_stopping\n",
    "        self.validation_fraction = validation_fraction\n",
    "        self.address = address\n",
    "        self._extra_train_ops = []\n",
    "        self.random_seed = random_seed\n",
    "        self.graph = tf.Graph()\n",
    "        self.is_built = False\n",
    "        with self.graph.as_default():\n",
    "            config = tf.ConfigProto()\n",
    "            config.gpu_options.allow_growth=True\n",
    "            self.sess = tf.Session(config=config)\n",
    "            \n",
    "    def _conv(self, x, filter_size, out_filters, strides, activation=None):\n",
    "        \n",
    "        in_filters = int(x.get_shape()[-1])\n",
    "        n = filter_size * filter_size * out_filters\n",
    "        kernel = tf.get_variable(\n",
    "            'DW', [filter_size, filter_size, in_filters, out_filters],\n",
    "            tf.float32, initializer=tf.random_normal_initializer(\n",
    "                stddev=np.sqrt(2.0/n)))\n",
    "        self.kernels.append(kernel)\n",
    "        x = tf.nn.conv2d(x, kernel, strides, padding='SAME')\n",
    "        if activation:\n",
    "            x = activation(x)\n",
    "        return x\n",
    "    \n",
    "    def _stride_arr(self, stride):\n",
    "        \n",
    "        if isinstance(stride, int):\n",
    "            return [1, stride, stride, 1]\n",
    "        if len(stride)==2:\n",
    "            return [1, stride[0], stride[1], 1]\n",
    "        if len(stride)==4:\n",
    "            return stride\n",
    "        raise ValueError('Invalid value!')  \n",
    "        \n",
    "    def _build_model(self, X, y):\n",
    "        \n",
    "        \n",
    "        if self.initializer is None:\n",
    "            initializer = tf.initializers.variance_scaling(distribution='uniform')\n",
    "        if self.activation is None:\n",
    "            activation = lambda x: tf.nn.relu(x)\n",
    "        self.input_ph = tf.placeholder(dtype=tf.float32, shape=(None,) + X.shape[1:], name='input')\n",
    "        self.dropout_ph = tf.placeholder_with_default(\n",
    "            tf.constant(0., dtype=tf.float32), shape=(), name='dropout')\n",
    "        if self.mode=='regression':\n",
    "            self.labels = tf.placeholder(dtype=tf.float32, shape=(None, ), name='label')\n",
    "        else:\n",
    "            self.labels = tf.placeholder(dtype=tf.int32, shape=(None, ), name='label')\n",
    "        if len(X.shape[1:]) == 2:\n",
    "            x = tf.reshape(self.input_ph, [-1, X.shape[0], X.shape[1], 1])\n",
    "        else:\n",
    "            x = self.input_ph\n",
    "        for layer, (kernel_size, channels, stride) in enumerate(zip(\n",
    "            self.kernel_sizes, self.channels, self.strides)):\n",
    "            with tf.variable_scope('conv_{}'.format(layer)):\n",
    "                x = self._conv(x, kernel_size, channels, self._stride_arr(stride), activation=activation)\n",
    "        if self.global_averaging:\n",
    "            x = tf.reduce_mean(x, axis=(1,2))\n",
    "        else:\n",
    "            x = tf.reshape(x, shape=(-1, np.prod(x.get_shape()[1:])))\n",
    "        for layer, hidden_unit in enumerate(self.hidden_units):\n",
    "            with tf.variable_scope('dense_{}'.format(layer)):\n",
    "                x = self._dense(x, hidden_unit, dropout=self.dropout_ph, \n",
    "                           initializer=self.initializer, activation=activation)\n",
    "                \n",
    "        with tf.variable_scope('final'):\n",
    "            self.prelogits = x\n",
    "            self._final_layer(self.prelogits, len(set(y)), self.mode)\n",
    "        self._build_train_op()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1cccbf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key text.latex.unicode in file /dartfs-hpc/rc/home/d/f0036rd/.conda/envs/SHAPenv/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 112 ('text.latex.unicode : False # use \"ucs\" and \"inputenc\" LaTeX packages for handling')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key text.latex.preview in file /dartfs-hpc/rc/home/d/f0036rd/.conda/envs/SHAPenv/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 125 ('text.latex.preview : False')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key mathtext.fallback_to_cm in file /dartfs-hpc/rc/home/d/f0036rd/.conda/envs/SHAPenv/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 157 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key savefig.jpeg_quality in file /dartfs-hpc/rc/home/d/f0036rd/.conda/envs/SHAPenv/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 420 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key savefig.frameon in file /dartfs-hpc/rc/home/d/f0036rd/.conda/envs/SHAPenv/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 423 ('savefig.frameon : True')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key pgf.debug in file /dartfs-hpc/rc/home/d/f0036rd/.conda/envs/SHAPenv/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 444 ('pgf.debug           : False')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key verbose.level in file /dartfs-hpc/rc/home/d/f0036rd/.conda/envs/SHAPenv/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 475 ('verbose.level  : silent      # one of silent, helpful, debug, debug-annoying')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key verbose.fileo in file /dartfs-hpc/rc/home/d/f0036rd/.conda/envs/SHAPenv/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 476 ('verbose.fileo  : sys.stdout  # a log filename, sys.stdout or sys.stderr')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key keymap.all_axes in file /dartfs-hpc/rc/home/d/f0036rd/.conda/envs/SHAPenv/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 493 ('keymap.all_axes : a                 # enable all axes')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key animation.avconv_path in file /dartfs-hpc/rc/home/d/f0036rd/.conda/envs/SHAPenv/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 504 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key animation.avconv_args in file /dartfs-hpc/rc/home/d/f0036rd/.conda/envs/SHAPenv/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 506 ('animation.avconv_args:            # Additional arguments to pass to avconv')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import inspect\n",
    "from scipy.stats import logistic\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.base import clone\n",
    "import inspect\n",
    "#from Shapley import ShapNN, CShapNN\n",
    "from multiprocessing import dummy as multiprocessing\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "        \n",
    "def convergence_plots(marginals):\n",
    "    \n",
    "    plt.rcParams['figure.figsize'] = 15,15\n",
    "    for i, idx in enumerate(np.arange(min(25, marginals.shape[-1]))):\n",
    "        plt.subplot(5,5,i+1)\n",
    "        plt.plot(np.cumsum(marginals[:, idx])/np.arange(1, len(marginals)+1))    \n",
    "        \n",
    "    \n",
    "def is_integer(array):\n",
    "    return (np.equal(np.mod(array, 1), 0).mean()==1)\n",
    "\n",
    "\n",
    "def is_fitted(model):\n",
    "        \"\"\"Checks if model object has any attributes ending with an underscore\"\"\"\n",
    "        return 0 < len( [k for k,v in inspect.getmembers(model) if k.endswith('_') and not k.startswith('__')] )\n",
    "\n",
    "\n",
    "def return_model(mode, **kwargs):\n",
    "    \n",
    "    \n",
    "    if inspect.isclass(mode):\n",
    "        assert getattr(mode, 'fit', None) is not None, 'Custom model family should have a fit() method'\n",
    "        model = mode(**kwargs)\n",
    "    elif mode=='logistic':\n",
    "        solver = kwargs.get('solver', 'liblinear')\n",
    "        n_jobs = kwargs.get('n_jobs', None)\n",
    "        max_iter = kwargs.get('max_iter', 5000)\n",
    "        model = LogisticRegression(solver=solver, n_jobs=n_jobs, \n",
    "                                 max_iter=max_iter, random_state=666)\n",
    "    elif mode=='Tree':\n",
    "        model = DecisionTreeClassifier(random_state=666)\n",
    "    elif mode=='RandomForest':\n",
    "        n_estimators = kwargs.get('n_estimators', 50)\n",
    "        model = RandomForestClassifier(n_estimators=n_estimators, random_state=666)\n",
    "    elif mode=='GB':\n",
    "        n_estimators = kwargs.get('n_estimators', 50)\n",
    "        model = GradientBoostingClassifier(n_estimators=n_estimators, random_state=666)\n",
    "    elif mode=='AdaBoost':\n",
    "        n_estimators = kwargs.get('n_estimators', 50)\n",
    "        model = AdaBoostClassifier(n_estimators=n_estimators, random_state=666)\n",
    "    elif mode=='SVC':\n",
    "        kernel = kwargs.get('kernel', 'rbf')\n",
    "        model = SVC(kernel=kernel, random_state=666)\n",
    "    elif mode=='LinearSVC':\n",
    "        model = LinearSVC(loss='hinge', random_state=666)\n",
    "    elif mode=='GP':\n",
    "        model = GaussianProcessClassifier(random_state=666)\n",
    "    elif mode=='KNN':\n",
    "        n_neighbors = kwargs.get('n_neighbors', 5)\n",
    "        model = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    elif mode=='NB':\n",
    "        model = MultinomialNB()\n",
    "    elif mode=='linear':\n",
    "        model = LinearRegression(random_state=666)\n",
    "    elif mode=='ridge':\n",
    "        alpha = kwargs.get('alpha', 1.0)\n",
    "        model = Ridge(alpha=alpha, random_state=666)\n",
    "    elif 'conv' in mode:\n",
    "        tf.reset_default_graph()\n",
    "        address = kwargs.get('address', 'weights/conv')\n",
    "        hidden_units = kwargs.get('hidden_layer_sizes', [20])\n",
    "        activation = kwargs.get('activation', 'relu')\n",
    "        weight_decay = kwargs.get('weight_decay', 1e-4)\n",
    "        learning_rate = kwargs.get('learning_rate', 0.001)\n",
    "        max_iter = kwargs.get('max_iter', 1000)\n",
    "        early_stopping= kwargs.get('early_stopping', 10)\n",
    "        warm_start = kwargs.get('warm_start', False)\n",
    "        batch_size = kwargs.get('batch_size', 256)\n",
    "        kernel_sizes = kwargs.get('kernel_sizes', [5])\n",
    "        strides = kwargs.get('strides', [5])\n",
    "        channels = kwargs.get('channels', [1])\n",
    "        validation_fraction = kwargs.get('validation_fraction', 0.)\n",
    "        global_averaging = kwargs.get('global_averaging', 0.)\n",
    "        optimizer = kwargs.get('optimizer', 'sgd')\n",
    "        if mode=='conv':\n",
    "            model = CShapNN(mode='classification', batch_size=batch_size, max_epochs=max_iter,\n",
    "                          learning_rate=learning_rate, \n",
    "                          weight_decay=weight_decay, validation_fraction=validation_fraction,\n",
    "                          early_stopping=early_stopping,\n",
    "                         optimizer=optimizer, warm_start=warm_start, address=address,\n",
    "                          hidden_units=hidden_units,\n",
    "                          strides=strides, global_averaging=global_averaging,\n",
    "                         kernel_sizes=kernel_sizes, channels=channels, random_seed=666)\n",
    "        elif mode=='conv_reg':\n",
    "            model = CShapNN(mode='regression', batch_size=batch_size, max_epochs=max_iter,\n",
    "                          learning_rate=learning_rate, \n",
    "                          weight_decay=weight_decay, validation_fraction=validation_fraction,\n",
    "                          early_stopping=early_stopping,\n",
    "                         optimizer=optimizer, warm_start=warm_start, address=address,\n",
    "                          hidden_units=hidden_units,\n",
    "                          strides=strides, global_averaging=global_averaging,\n",
    "                         kernel_sizes=kernel_sizes, channels=channels, random_seed=666)\n",
    "    elif 'NN' in mode:\n",
    "        solver = kwargs.get('solver', 'adam')\n",
    "        hidden_layer_sizes = kwargs.get('hidden_layer_sizes', (20,))\n",
    "        if isinstance(hidden_layer_sizes, list):\n",
    "            hidden_layer_sizes = list(hidden_layer_sizes)\n",
    "        activation = kwargs.get('activation', 'relu')\n",
    "        learning_rate_init = kwargs.get('learning_rate', 0.001)\n",
    "        max_iter = kwargs.get('max_iter', 5000)\n",
    "        early_stopping= kwargs.get('early_stopping', False)\n",
    "        warm_start = kwargs.get('warm_start', False)\n",
    "        if mode=='NN':\n",
    "            model = MLPClassifier(solver=solver, hidden_layer_sizes=hidden_layer_sizes,\n",
    "                                activation=activation, learning_rate_init=learning_rate_init,\n",
    "                                warm_start = warm_start, max_iter=max_iter,\n",
    "                                early_stopping=early_stopping)\n",
    "        if mode=='NN_reg':\n",
    "            model = MLPRegressor(solver=solver, hidden_layer_sizes=hidden_layer_sizes,\n",
    "                                activation=activation, learning_rate_init=learning_rate_init,\n",
    "                                warm_start = warm_start, max_iter=max_iter, early_stopping=early_stopping)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid mode!\")\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def generate_features(latent, dependency):\n",
    "\n",
    "    features = []\n",
    "    n = latent.shape[0]\n",
    "    exp = latent\n",
    "    holder = latent\n",
    "    for order in range(1,dependency+1):\n",
    "        features.append(np.reshape(holder,[n,-1]))\n",
    "        exp = np.expand_dims(exp,-1)\n",
    "        holder = exp * np.expand_dims(holder,1)\n",
    "    return np.concatenate(features,axis=-1)  \n",
    "\n",
    "\n",
    "def label_generator(problem, X, param, difficulty=1, beta=None, important=None):\n",
    "        \n",
    "    if important is None or important > X.shape[-1]:\n",
    "        important = X.shape[-1]\n",
    "    dim_latent = sum([important**i for i in range(1, difficulty+1)])\n",
    "    if beta is None:\n",
    "        beta = np.random.normal(size=[1, dim_latent])\n",
    "    important_dims = np.random.choice(X.shape[-1], important, replace=False)\n",
    "    funct_init = lambda inp: np.sum(beta * generate_features(inp[:,important_dims], difficulty), -1)\n",
    "    batch_size = max(100, min(len(X), 10000000//dim_latent))\n",
    "    y_true = np.zeros(len(X))\n",
    "    while True:\n",
    "        try:\n",
    "            for itr in range(int(np.ceil(len(X)/batch_size))):\n",
    "                y_true[itr * batch_size: (itr+1) * batch_size] = funct_init(\n",
    "                    X[itr * batch_size: (itr+1) * batch_size])\n",
    "            break\n",
    "        except MemoryError:\n",
    "            batch_size = batch_size//2\n",
    "    mean, std = np.mean(y_true), np.std(y_true)\n",
    "    funct = lambda x: (np.sum(beta * generate_features(\n",
    "        x[:, important_dims], difficulty), -1) - mean) / std\n",
    "    y_true = (y_true - mean)/std\n",
    "    if problem is 'classification':\n",
    "        y_true = logistic.cdf(param * y_true)\n",
    "        y = (np.random.random(X.shape[0]) < y_true).astype(int)\n",
    "    elif problem is 'regression':\n",
    "        y = y_true + param * np.random.normal(size=len(y_true))\n",
    "    else:\n",
    "        raise ValueError('Invalid problem specified!')\n",
    "    return beta, y, y_true, funct\n",
    "\n",
    "\n",
    "def one_iteration(clf, X, y, X_test, y_test, mean_score, tol=0.0, c=None, metric='accuracy'):\n",
    "    \"\"\"Runs one iteration of TMC-Shapley.\"\"\"\n",
    "    \n",
    "    if metric == 'auc':\n",
    "        def score_func(clf, a, b):\n",
    "            return roc_auc_score(b, clf.predict_proba(a)[:,1])\n",
    "    elif metric == 'accuracy':\n",
    "        def score_func(clf, a, b):\n",
    "            return clf.score(a, b)\n",
    "    else:\n",
    "        raise ValueError(\"Wrong metric!\")  \n",
    "    if c is None:\n",
    "        c = {i:np.array([i]) for i in range(len(X))}\n",
    "    idxs, marginal_contribs = np.random.permutation(len(c.keys())), np.zeros(len(X))\n",
    "    new_score = np.max(np.bincount(y)) * 1./len(y) if np.mean(y//1 == y/1)==1 else 0.\n",
    "    start = 0\n",
    "    if start:\n",
    "        X_batch, y_batch =\\\n",
    "        np.concatenate([X[c[idx]] for idx in idxs[:start]]), np.concatenate([y[c[idx]] for idx in idxs[:start]])\n",
    "    else:\n",
    "        X_batch, y_batch = np.zeros((0,) +  tuple(X.shape[1:])), np.zeros(0).astype(int)\n",
    "    for n, idx in enumerate(idxs[start:]):\n",
    "        try:\n",
    "            clf = clone(clf)\n",
    "        except:\n",
    "            clf.fit(np.zeros((0,) +  X.shape[1:]), y)\n",
    "        old_score = new_score\n",
    "        X_batch, y_batch = np.concatenate([X_batch, X[c[idx]]]), np.concatenate([y_batch, y[c[idx]]])\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            try:\n",
    "                clf.fit(X_batch, y_batch)\n",
    "                temp_score = score_func(clf, X_test, y_test)\n",
    "                if temp_score>-1 and temp_score<1.: #Removing measningless r2 scores\n",
    "                    new_score = temp_score\n",
    "            except:\n",
    "                continue\n",
    "        marginal_contribs[c[idx]] = (new_score - old_score)/len(c[idx])\n",
    "        if np.abs(new_score - mean_score)/mean_score < tol:\n",
    "            break\n",
    "    return marginal_contribs, idxs\n",
    "\n",
    "\n",
    "def marginals(clf, X, y, X_test, y_test, c=None, tol=0., trials=3000, mean_score=None, metric='accuracy'):\n",
    "    \n",
    "    if metric == 'auc':\n",
    "        def score_func(clf, a, b):\n",
    "            return roc_auc_score(b, clf.predict_proba(a)[:,1])\n",
    "    elif metric == 'accuracy':\n",
    "        def score_func(clf, a, b):\n",
    "            return clf.score(a, b)\n",
    "    else:\n",
    "        raise ValueError(\"Wrong metric!\")  \n",
    "    if mean_score is None:\n",
    "        accs = []\n",
    "        for _ in range(100):\n",
    "            bag_idxs = np.random.choice(len(y_test), len(y_test))\n",
    "            accs.append(score_func(clf, X_test[bag_idxs], y_test[bag_idxs]))\n",
    "        mean_score = np.mean(accs)\n",
    "    marginals, idxs = [], []\n",
    "    for trial in range(trials):\n",
    "        if 10*(trial+1)/trials % 1 == 0:\n",
    "            print('{} out of {}'.format(trial + 1, trials))\n",
    "        marginal, idx = one_iteration(clf, X, y, X_test, y_test, mean_score, tol=tol, c=c, metric=metric)\n",
    "        marginals.append(marginal)\n",
    "        idxs.append(idx)\n",
    "    return np.array(marginals), np.array(idxs)\n",
    "\n",
    "def shapley(mode, X, y, X_test, y_test, stop=None, tol=0., trials=3000, **kwargs):\n",
    "    \n",
    "    try:\n",
    "        vals = np.zeros(len(X))\n",
    "        example_idxs = np.random.choice(len(X), min(25, len(X)), replace=False)\n",
    "        example_marginals = np.zeros((trials, len(example_idxs)))\n",
    "        for i in range(trials):\n",
    "            print(i)\n",
    "            output = one_pass(mode, X, y, X_test, y_test, tol=tol, stop=stop, **kwargs)\n",
    "            example_marginals[i] = output[0][example_idxs]\n",
    "            vals = vals/(i+1) + output[0]/(i+1)\n",
    "        return vals, example_marginals\n",
    "    except KeyboardInterrupt:\n",
    "        print('Interrupted!')\n",
    "        return vals, example_marginals\n",
    "\n",
    "def early_stopping(marginals, idxs, stopping):\n",
    "    \n",
    "    stopped_marginals = np.zeros_like(marginals)\n",
    "    for i in range(len(marginals)):\n",
    "        stopped_marginals[i][idxs[i][:stopping]] = marginals[i][idxs[i][:stopping]]\n",
    "    return np.mean(stopped_marginals, 0)\n",
    "\n",
    "def error(mem):\n",
    "    \n",
    "    if len(mem) < 100:\n",
    "        return 1.0\n",
    "    all_vals = (np.cumsum(mem, 0)/np.reshape(np.arange(1, len(mem)+1), (-1,1)))[-100:]\n",
    "    errors = np.mean(np.abs(all_vals[-100:] - all_vals[-1:])/(np.abs(all_vals[-1:]) + 1e-12), -1)\n",
    "    return np.max(errors)\n",
    "\n",
    "def my_accuracy_score(clf, X, y):\n",
    "    \n",
    "    probs = clf.predict_proba(X)\n",
    "    predictions = np.argmax(probs, -1)\n",
    "    return np.mean(np.equal(predictions, y))\n",
    "\n",
    "def my_f1_score(clf, X, y):\n",
    "    \n",
    "    predictions = clf.predict(x)\n",
    "    if len(set(y)) == 2:\n",
    "        return f1_score(y, predictions)\n",
    "    return f1_score(y, predictions, average='macro')\n",
    "\n",
    "def my_auc_score(clf, X, y):\n",
    "    \n",
    "    probs = clf.predict_proba(X)\n",
    "    true_probs = probs[np.arange(len(y)), y]\n",
    "    return roc_auc_score(y, true_probs)\n",
    "\n",
    "def my_xe_score(clf, X, y):\n",
    "    \n",
    "    probs = clf.predict_proba(X)\n",
    "    true_probs = probs[np.arange(len(y)), y]\n",
    "    true_log_probs = np.log(np.clip(true_probs, 1e-12, None))\n",
    "    return np.mean(true_log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22d9ce35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#______________________________________PEP8____________________________________\n",
    "#_______________________________________________________________________\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "#from shap_utils import *\n",
    "#from Shapley import ShapNN\n",
    "from scipy.stats import spearmanr\n",
    "#import shutil\n",
    "from sklearn.base import clone\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import itertools\n",
    "import inspect\n",
    "import _pickle as pkl\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "class DShap(object):\n",
    "    \n",
    "    def __init__(self, X, y, X_test, y_test, num_test, sources=None, \n",
    "                 sample_weight=None, directory=None, problem='classification',\n",
    "                 model_family='logistic', metric='accuracy', seed=None,\n",
    "                 overwrite=False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X: Data covariates\n",
    "            y: Data labels\n",
    "            X_test: Test+Held-out covariates\n",
    "            y_test: Test+Held-out labels\n",
    "            sources: An array or dictionary assiging each point to its group.\n",
    "                If None, evey points gets its individual value.\n",
    "            samples_weights: Weight of train samples in the loss function\n",
    "                (for models where weighted training method is enabled.)\n",
    "            num_test: Number of data points used for evaluation metric.\n",
    "            directory: Directory to save results and figures.\n",
    "            problem: \"Classification\" or \"Regression\"(Not implemented yet.)\n",
    "            model_family: The model family used for learning algorithm\n",
    "            metric: Evaluation metric\n",
    "            seed: Random seed. When running parallel monte-carlo samples,\n",
    "                we initialize each with a different seed to prevent getting \n",
    "                same permutations.\n",
    "            overwrite: Delete existing data and start computations from \n",
    "                scratch\n",
    "            **kwargs: Arguments of the model\n",
    "        \"\"\"\n",
    "            \n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "            tf.random.set_random_seed(seed)\n",
    "        self.problem = problem\n",
    "        self.model_family = model_family\n",
    "        self.metric = metric\n",
    "        self.directory = directory\n",
    "        self.hidden_units = kwargs.get('hidden_layer_sizes', [])\n",
    "        if self.model_family is 'logistic':\n",
    "            self.hidden_units = []\n",
    "        if self.directory is not None:\n",
    "            if overwrite and os.path.exists(directory):\n",
    "                tf.gfile.DeleteRecursively(directory)\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)  \n",
    "                os.makedirs(os.path.join(directory, 'weights'))\n",
    "                os.makedirs(os.path.join(directory, 'plots'))\n",
    "            self._initialize_instance(X, y, X_test, y_test, num_test, \n",
    "                                      sources, sample_weight)\n",
    "        if len(set(self.y)) > 2:\n",
    "            assert self.metric != 'f1', 'Invalid metric for multiclass!'\n",
    "            assert self.metric != 'auc', 'Invalid metric for multiclass!'\n",
    "        is_regression = (np.mean(self.y//1 == self.y) != 1)\n",
    "        is_regression = is_regression or isinstance(self.y[0], np.float32)\n",
    "        self.is_regression = is_regression or isinstance(self.y[0], np.float64)\n",
    "        if self.is_regression:\n",
    "            warnings.warn(\"Regression problem is no implemented.\")\n",
    "        self.model = return_model(self.model_family, **kwargs)\n",
    "        self.random_score = self.init_score(self.metric)\n",
    "            \n",
    "    def _initialize_instance(self, X, y, X_test, y_test, num_test, \n",
    "                             sources=None, sample_weight=None):\n",
    "        \"\"\"Loads or creates sets of data.\"\"\"      \n",
    "        if sources is None:\n",
    "            sources = {i:np.array([i]) for i in range(len(X))}\n",
    "        elif not isinstance(sources, dict):\n",
    "            sources = {i:np.where(sources==i)[0] for i in set(sources)}\n",
    "        data_dir = os.path.join(self.directory, 'data.pkl')\n",
    "        if os.path.exists(data_dir):\n",
    "            self._load_dataset(data_dir)\n",
    "        else:\n",
    "            self.X_heldout = X_test[:-num_test]\n",
    "            self.y_heldout = y_test[:-num_test]\n",
    "            self.X_test = X_test[-num_test:]\n",
    "            self.y_test = y_test[-num_test:]\n",
    "            self.X, self.y, self.sources = X, y, sources\n",
    "            self.sample_weight = sample_weight\n",
    "            data_dic = {'X': self.X, 'y': self.y, 'X_test': self.X_test,\n",
    "                     'y_test': self.y_test, 'X_heldout': self.X_heldout,\n",
    "                     'y_heldout':self.y_heldout, 'sources': self.sources}\n",
    "            if sample_weight is not None:\n",
    "                data_dic['sample_weight'] = sample_weight\n",
    "                warnings.warn(\"Sample weight not implemented for G-Shapley\")\n",
    "            pkl.dump(data_dic, open(data_dir, 'wb'))        \n",
    "        loo_dir = os.path.join(self.directory, 'loo.pkl')\n",
    "        self.vals_loo = None\n",
    "        if os.path.exists(loo_dir):\n",
    "            self.vals_loo = pkl.load(open(loo_dir, 'rb'))['loo']\n",
    "        n_sources = len(self.X) if self.sources is None else len(self.sources)\n",
    "        n_points = len(self.X)\n",
    "        self.tmc_number, self.g_number = self._which_parallel(self.directory)\n",
    "        self._create_results_placeholder(\n",
    "            self.directory, self.tmc_number, self.g_number,\n",
    "            n_points, n_sources, self.model_family)\n",
    "        \n",
    "    def _create_results_placeholder(self, directory, tmc_number, g_number,\n",
    "                                   n_points, n_sources, model_family):\n",
    "        tmc_dir = os.path.join(\n",
    "            directory, \n",
    "            'mem_tmc_{}.pkl'.format(tmc_number.zfill(4))\n",
    "        )\n",
    "        g_dir = os.path.join(\n",
    "            directory, \n",
    "            'mem_g_{}.pkl'.format(g_number.zfill(4))\n",
    "        )\n",
    "        self.mem_tmc = np.zeros((0, n_points))\n",
    "        self.mem_g = np.zeros((0, n_points))\n",
    "        self.idxs_tmc = np.zeros((0, n_sources), int)\n",
    "        self.idxs_g = np.zeros((0, n_sources), int)\n",
    "        pkl.dump({'mem_tmc': self.mem_tmc, 'idxs_tmc': self.idxs_tmc}, \n",
    "                 open(tmc_dir, 'wb'))\n",
    "        if model_family not in ['logistic', 'NN']:\n",
    "            return\n",
    "        pkl.dump({'mem_g': self.mem_g, 'idxs_g': self.idxs_g}, \n",
    "                 open(g_dir, 'wb'))\n",
    "        \n",
    "    def _load_dataset(self, data_dir):\n",
    "        '''Load the different sets of data if already exists.'''\n",
    "        data_dic = pkl.load(open(data_dir, 'rb'))\n",
    "        self.X_heldout = data_dic['X_heldout']\n",
    "        self.y_heldout = data_dic['y_heldout']\n",
    "        self.X_test = data_dic['X_test']\n",
    "        self.y_test = data_dic['y_test']\n",
    "        self.X = data_dic['X'] \n",
    "        self.y = data_dic['y']\n",
    "        self.sources = data_dic['sources']\n",
    "        if 'sample_weight' in data_dic.keys():\n",
    "            self.sample_weight = data_dic['sample_weight']\n",
    "        else:\n",
    "            self.sample_weight = None\n",
    "        \n",
    "    def _which_parallel(self, directory):\n",
    "        '''Prevent conflict with parallel runs.'''\n",
    "        previous_results = os.listdir(directory)\n",
    "        tmc_nmbrs = [int(name.split('.')[-2].split('_')[-1])\n",
    "                      for name in previous_results if 'mem_tmc' in name]\n",
    "        g_nmbrs = [int(name.split('.')[-2].split('_')[-1])\n",
    "                     for name in previous_results if 'mem_g' in name]        \n",
    "        tmc_number = str(np.max(tmc_nmbrs) + 1) if len(tmc_nmbrs) else '0' \n",
    "        g_number = str(np.max(g_nmbrs) + 1) if len(g_nmbrs) else '0' \n",
    "        return tmc_number, g_number\n",
    "    \n",
    "    def init_score(self, metric):\n",
    "        \"\"\" Gives the value of an initial untrained model.\"\"\"\n",
    "        if metric == 'accuracy':\n",
    "            hist = np.bincount(self.y_test).astype(float)/len(self.y_test)\n",
    "            return np.max(hist)\n",
    "        if metric == 'f1':\n",
    "            rnd_f1s = []\n",
    "            for _ in range(1000):\n",
    "                rnd_y = np.random.permutation(self.y_test)\n",
    "                rnd_f1s.append(f1_score(self.y_test, rnd_y))\n",
    "            return np.mean(rnd_f1s)\n",
    "        if metric == 'auc':\n",
    "            return 0.5\n",
    "        random_scores = []\n",
    "        for _ in range(100):\n",
    "            rnd_y = np.random.permutation(self.y)\n",
    "            if self.sample_weight is None:\n",
    "                self.model.fit(self.X, rnd_y)\n",
    "            else:\n",
    "                self.model.fit(self.X, rnd_y, \n",
    "                               sample_weight=self.sample_weight)\n",
    "            random_scores.append(self.value(self.model, metric))\n",
    "        return np.mean(random_scores)\n",
    "        \n",
    "    def value(self, model, metric=None, X=None, y=None):\n",
    "        \"\"\"Computes the values of the given model.\n",
    "        Args:\n",
    "            model: The model to be evaluated.\n",
    "            metric: Valuation metric. If None the object's default\n",
    "                metric is used.\n",
    "            X: Covariates, valuation is performed on a data \n",
    "                different from test set.\n",
    "            y: Labels, if valuation is performed on a data \n",
    "                different from test set.\n",
    "            \"\"\"\n",
    "        if metric is None:\n",
    "            metric = self.metric\n",
    "        if X is None:\n",
    "            X = self.X_test\n",
    "        if y is None:\n",
    "            y = self.y_test\n",
    "        if inspect.isfunction(metric):\n",
    "            return metric(model, X, y)\n",
    "        if metric == 'accuracy':\n",
    "            return model.score(X, y)\n",
    "        if metric == 'f1':\n",
    "            assert len(set(y)) == 2, 'Data has to be binary for f1 metric.'\n",
    "            return f1_score(y, model.predict(X))\n",
    "        if metric == 'auc':\n",
    "            assert len(set(y)) == 2, 'Data has to be binary for auc metric.'\n",
    "            return my_auc_score(model, X, y)\n",
    "        if metric == 'xe':\n",
    "            return my_xe_score(model, X, y)\n",
    "        raise ValueError('Invalid metric!')\n",
    "        \n",
    "    def run(self, save_every, err, tolerance=0.01, g_run=True, loo_run=True):\n",
    "        \"\"\"Calculates data sources(points) values.\n",
    "        \n",
    "        Args:\n",
    "            save_every: save marginal contrivbutions every n iterations.\n",
    "            err: stopping criteria.\n",
    "            tolerance: Truncation tolerance. If None, it's computed.\n",
    "            g_run: If True, computes G-Shapley values.\n",
    "            loo_run: If True, computes and saves leave-one-out scores.\n",
    "        \"\"\"\n",
    "        if loo_run:\n",
    "            try:\n",
    "                len(self.vals_loo)\n",
    "            except:\n",
    "                self.vals_loo = self._calculate_loo_vals(sources=self.sources)\n",
    "                self.save_results(overwrite=True)\n",
    "        print('LOO values calculated!')\n",
    "        tmc_run = True \n",
    "        g_run = g_run and self.model_family in ['logistic', 'NN']\n",
    "        while tmc_run or g_run:\n",
    "            if g_run:\n",
    "                if error(self.mem_g) < err:\n",
    "                    g_run = False\n",
    "                else:\n",
    "                    self._g_shap(save_every, sources=self.sources)\n",
    "                    self.vals_g = np.mean(self.mem_g, 0)\n",
    "            if tmc_run:\n",
    "                if error(self.mem_tmc) < err:\n",
    "                    tmc_run = False\n",
    "                else:\n",
    "                    self._tmc_shap(\n",
    "                        save_every, \n",
    "                        tolerance=tolerance, \n",
    "                        sources=self.sources\n",
    "                    )\n",
    "                    self.vals_tmc = np.mean(self.mem_tmc, 0)\n",
    "            if self.directory is not None:\n",
    "                self.save_results()\n",
    "            \n",
    "    def save_results(self, overwrite=False):\n",
    "        \"\"\"Saves results computed so far.\"\"\"\n",
    "        if self.directory is None:\n",
    "            return\n",
    "        loo_dir = os.path.join(self.directory, 'loo.pkl')\n",
    "        if not os.path.exists(loo_dir) or overwrite:\n",
    "            pkl.dump({'loo': self.vals_loo}, open(loo_dir, 'wb'))\n",
    "        tmc_dir = os.path.join(\n",
    "            self.directory, \n",
    "            'mem_tmc_{}.pkl'.format(self.tmc_number.zfill(4))\n",
    "        )\n",
    "        g_dir = os.path.join(\n",
    "            self.directory, \n",
    "            'mem_g_{}.pkl'.format(self.g_number.zfill(4))\n",
    "        )  \n",
    "        pkl.dump({'mem_tmc': self.mem_tmc, 'idxs_tmc': self.idxs_tmc}, \n",
    "                 open(tmc_dir, 'wb'))\n",
    "        pkl.dump({'mem_g': self.mem_g, 'idxs_g': self.idxs_g}, \n",
    "                 open(g_dir, 'wb'))  \n",
    "        \n",
    "    def _tmc_shap(self, iterations, tolerance=None, sources=None):\n",
    "        \"\"\"Runs TMC-Shapley algorithm.\n",
    "        \n",
    "        Args:\n",
    "            iterations: Number of iterations to run.\n",
    "            tolerance: Truncation tolerance ratio.\n",
    "            sources: If values are for sources of data points rather than\n",
    "                   individual points. In the format of an assignment array\n",
    "                   or dict.\n",
    "        \"\"\"\n",
    "        if sources is None:\n",
    "            sources = {i: np.array([i]) for i in range(len(self.X))}\n",
    "        elif not isinstance(sources, dict):\n",
    "            sources = {i: np.where(sources == i)[0] for i in set(sources)}\n",
    "        model = self.model\n",
    "        try:\n",
    "            self.mean_score\n",
    "        except:\n",
    "            self._tol_mean_score()\n",
    "        if tolerance is None:\n",
    "            tolerance = self.tolerance         \n",
    "        marginals, idxs = [], []\n",
    "        for iteration in range(iterations):\n",
    "            if 10*(iteration+1)/iterations % 1 == 0:\n",
    "                print('{} out of {} TMC_Shapley iterations.'.format(\n",
    "                    iteration + 1, iterations))\n",
    "            marginals, idxs = self.one_iteration(\n",
    "                tolerance=tolerance, \n",
    "                sources=sources\n",
    "            )\n",
    "            self.mem_tmc = np.concatenate([\n",
    "                self.mem_tmc, \n",
    "                np.reshape(marginals, (1,-1))\n",
    "            ])\n",
    "            self.idxs_tmc = np.concatenate([\n",
    "                self.idxs_tmc, \n",
    "                np.reshape(idxs, (1,-1))\n",
    "            ])\n",
    "        \n",
    "    def _tol_mean_score(self):\n",
    "        \"\"\"Computes the average performance and its error using bagging.\"\"\"\n",
    "        scores = []\n",
    "        self.restart_model()\n",
    "        for _ in range(1):\n",
    "            if self.sample_weight is None:\n",
    "                self.model.fit(self.X, self.y)\n",
    "            else:\n",
    "                self.model.fit(self.X, self.y,\n",
    "                              sample_weight=self.sample_weight)\n",
    "            for _ in range(100):\n",
    "                bag_idxs = np.random.choice(len(self.y_test), len(self.y_test))\n",
    "                scores.append(self.value(\n",
    "                    self.model, \n",
    "                    metric=self.metric,\n",
    "                    X=self.X_test[bag_idxs], \n",
    "                    y=self.y_test[bag_idxs]\n",
    "                ))\n",
    "        self.tol = np.std(scores)\n",
    "        self.mean_score = np.mean(scores)\n",
    "        \n",
    "    def one_iteration(self, tolerance, sources=None):\n",
    "        \"\"\"Runs one iteration of TMC-Shapley algorithm.\"\"\"\n",
    "        if sources is None:\n",
    "            sources = {i: np.array([i]) for i in range(len(self.X))}\n",
    "        elif not isinstance(sources, dict):\n",
    "            sources = {i: np.where(sources == i)[0] for i in set(sources)}\n",
    "        idxs = np.random.permutation(len(sources))\n",
    "        marginal_contribs = np.zeros(len(self.X))\n",
    "        X_batch = np.zeros((0,) + tuple(self.X.shape[1:]))\n",
    "        y_batch = np.zeros(0, int)\n",
    "        sample_weight_batch = np.zeros(0)\n",
    "        truncation_counter = 0\n",
    "        new_score = self.random_score\n",
    "        for n, idx in enumerate(idxs):\n",
    "            old_score = new_score\n",
    "            X_batch = np.concatenate([X_batch, self.X[sources[idx]]])\n",
    "            y_batch = np.concatenate([y_batch, self.y[sources[idx]]])\n",
    "            if self.sample_weight is None:\n",
    "                sample_weight_batch = None\n",
    "            else:\n",
    "                sample_weight_batch = np.concatenate([\n",
    "                    sample_weight_batch, \n",
    "                    self.sample_weight[sources[idx]]\n",
    "                ])\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "                if (self.is_regression \n",
    "                    or len(set(y_batch)) == len(set(self.y_test))): ##FIXIT\n",
    "                    self.restart_model()\n",
    "                    if sample_weight_batch is None:\n",
    "                        self.model.fit(X_batch, y_batch)\n",
    "                    else:\n",
    "                        self.model.fit(\n",
    "                            X_batch, \n",
    "                            y_batch,\n",
    "                            sample_weight = sample_weight_batch\n",
    "                        )\n",
    "                    new_score = self.value(self.model, metric=self.metric)       \n",
    "            marginal_contribs[sources[idx]] = (new_score - old_score)\n",
    "            marginal_contribs[sources[idx]] /= len(sources[idx])\n",
    "            distance_to_full_score = np.abs(new_score - self.mean_score)\n",
    "            if distance_to_full_score <= tolerance * self.mean_score:\n",
    "                truncation_counter += 1\n",
    "                if truncation_counter > 5:\n",
    "                    break\n",
    "            else:\n",
    "                truncation_counter = 0\n",
    "        return marginal_contribs, idxs\n",
    "    \n",
    "    def restart_model(self):\n",
    "        \n",
    "        try:\n",
    "            self.model = clone(self.model)\n",
    "        except:\n",
    "            self.model.fit(np.zeros((0,) + self.X.shape[1:]), self.y)\n",
    "        \n",
    "    def _one_step_lr(self):\n",
    "        \"\"\"Computes the best learning rate for G-Shapley algorithm.\"\"\"\n",
    "        if self.directory is None:\n",
    "            address = None\n",
    "        else:\n",
    "            address = os.path.join(self.directory, 'weights')\n",
    "        best_acc = 0.0\n",
    "        for i in np.arange(1, 5, 0.5):\n",
    "            model = ShapNN(\n",
    "                self.problem, batch_size=1, max_epochs=1, \n",
    "                learning_rate=10**(-i), weight_decay=0., \n",
    "                validation_fraction=0, optimizer='sgd', \n",
    "                warm_start=False, address=address, \n",
    "                hidden_units=self.hidden_units)\n",
    "            accs = []\n",
    "            for _ in range(10):\n",
    "                model.fit(np.zeros((0, self.X.shape[-1])), self.y)\n",
    "                model.fit(self.X, self.y)\n",
    "                accs.append(model.score(self.X_test, self.y_test))\n",
    "            if np.mean(accs) - np.std(accs) > best_acc:\n",
    "                best_acc  = np.mean(accs) - np.std(accs)\n",
    "                learning_rate = 10**(-i)\n",
    "        return learning_rate\n",
    "    \n",
    "    def _g_shap(self, iterations, err=None, learning_rate=None, sources=None):\n",
    "        \"\"\"Method for running G-Shapley algorithm.\n",
    "        \n",
    "        Args:\n",
    "            iterations: Number of iterations of the algorithm.\n",
    "            err: Stopping error criteria\n",
    "            learning_rate: Learning rate used for the algorithm. If None\n",
    "                calculates the best learning rate.\n",
    "            sources: If values are for sources of data points rather than\n",
    "                   individual points. In the format of an assignment array\n",
    "                   or dict.\n",
    "        \"\"\"\n",
    "        if sources is None:\n",
    "            sources = {i:np.array([i]) for i in range(len(self.X))}\n",
    "        elif not isinstance(sources, dict):\n",
    "            sources = {i:np.where(sources==i)[0] for i in set(sources)}\n",
    "        address = None\n",
    "        if self.directory is not None:\n",
    "            address = os.path.join(self.directory, 'weights')\n",
    "        if learning_rate is None:\n",
    "            try:\n",
    "                learning_rate = self.g_shap_lr\n",
    "            except AttributeError:\n",
    "                self.g_shap_lr = self._one_step_lr()\n",
    "                learning_rate = self.g_shap_lr\n",
    "        model = ShapNN(self.problem, batch_size=1, max_epochs=1,\n",
    "                     learning_rate=learning_rate, weight_decay=0.,\n",
    "                     validation_fraction=0, optimizer='sgd',\n",
    "                     address=address, hidden_units=self.hidden_units)\n",
    "        for iteration in range(iterations):\n",
    "            model.fit(np.zeros((0, self.X.shape[-1])), self.y)\n",
    "            if 10 * (iteration+1) / iterations % 1 == 0:\n",
    "                print('{} out of {} G-Shapley iterations'.format(\n",
    "                    iteration + 1, iterations))\n",
    "            marginal_contribs = np.zeros(len(sources.keys()))\n",
    "            model.fit(self.X, self.y, self.X_test, self.y_test, \n",
    "                      sources=sources, metric=self.metric, \n",
    "                      max_epochs=1, batch_size=1)\n",
    "            val_result = model.history['metrics']\n",
    "            marginal_contribs[1:] += val_result[0][1:]\n",
    "            marginal_contribs[1:] -= val_result[0][:-1]\n",
    "            individual_contribs = np.zeros(len(self.X))\n",
    "            for i, index in enumerate(model.history['idxs'][0]):\n",
    "                individual_contribs[sources[index]] += marginal_contribs[i]\n",
    "                individual_contribs[sources[index]] /= len(sources[index])\n",
    "            self.mem_g = np.concatenate(\n",
    "                [self.mem_g, np.reshape(individual_contribs, (1,-1))])\n",
    "            self.idxs_g = np.concatenate(\n",
    "                [self.idxs_g, np.reshape(model.history['idxs'][0], (1,-1))])\n",
    "    \n",
    "    def _calculate_loo_vals(self, sources=None, metric=None):\n",
    "        \"\"\"Calculated leave-one-out values for the given metric.\n",
    "        \n",
    "        Args:\n",
    "            metric: If None, it will use the objects default metric.\n",
    "            sources: If values are for sources of data points rather than\n",
    "                   individual points. In the format of an assignment array\n",
    "                   or dict.\n",
    "        \n",
    "        Returns:\n",
    "            Leave-one-out scores\n",
    "        \"\"\"\n",
    "        if sources is None:\n",
    "            sources = {i:np.array([i]) for i in range(len(self.X))}\n",
    "        elif not isinstance(sources, dict):\n",
    "            sources = {i:np.where(sources==i)[0] for i in set(sources)}\n",
    "        print('Starting LOO score calculations!')\n",
    "        if metric is None:\n",
    "            metric = self.metric \n",
    "        self.restart_model()\n",
    "        if self.sample_weight is None:\n",
    "            self.model.fit(self.X, self.y)\n",
    "        else:\n",
    "            self.model.fit(self.X, self.y,\n",
    "                          sample_weight=self.sample_weight)\n",
    "        baseline_value = self.value(self.model, metric=metric)\n",
    "        vals_loo = np.zeros(len(self.X))\n",
    "        for i in sources.keys():\n",
    "            X_batch = np.delete(self.X, sources[i], axis=0)\n",
    "            y_batch = np.delete(self.y, sources[i], axis=0)\n",
    "            if self.sample_weight is not None:\n",
    "                sw_batch = np.delete(self.sample_weight, sources[i], axis=0)\n",
    "            if self.sample_weight is None:\n",
    "                self.model.fit(X_batch, y_batch)\n",
    "            else:\n",
    "                self.model.fit(X_batch, y_batch, sample_weight=sw_batch)\n",
    "                \n",
    "            removed_value = self.value(self.model, metric=metric)\n",
    "            vals_loo[sources[i]] = (baseline_value - removed_value)\n",
    "            vals_loo[sources[i]] /= len(sources[i])\n",
    "        return vals_loo\n",
    "    \n",
    "    def _merge_parallel_results(self, key, max_samples=None):\n",
    "        \"\"\"Helper method for 'merge_results' method.\"\"\"\n",
    "        numbers = [name.split('.')[-2].split('_')[-1]\n",
    "                   for name in os.listdir(self.directory) \n",
    "                   if 'mem_{}'.format(key) in name]\n",
    "        mem  = np.zeros((0, self.X.shape[0]))\n",
    "        n_sources = len(self.X) if self.sources is None else len(self.sources)\n",
    "        idxs = np.zeros((0, n_sources), int)\n",
    "        vals = np.zeros(len(self.X))\n",
    "        counter = 0.\n",
    "        for number in numbers:\n",
    "            if max_samples is not None:\n",
    "                if counter > max_samples:\n",
    "                    break\n",
    "            samples_dir = os.path.join(\n",
    "                self.directory, \n",
    "                'mem_{}_{}.pkl'.format(key, number)\n",
    "            )\n",
    "            print(samples_dir)\n",
    "            dic = pkl.load(open(samples_dir, 'rb'))\n",
    "            if not len(dic['mem_{}'.format(key)]):\n",
    "                continue\n",
    "            mem = np.concatenate([mem, dic['mem_{}'.format(key)]])\n",
    "            idxs = np.concatenate([idxs, dic['idxs_{}'.format(key)]])\n",
    "            counter += len(dic['mem_{}'.format(key)])\n",
    "            vals *= (counter - len(dic['mem_{}'.format(key)])) / counter\n",
    "            vals += len(dic['mem_{}'.format(key)]) / counter * np.mean(mem, 0)\n",
    "            os.remove(samples_dir)\n",
    "        merged_dir = os.path.join(\n",
    "            self.directory, \n",
    "            'mem_{}_0000.pkl'.format(key)\n",
    "        )\n",
    "        pkl.dump({'mem_{}'.format(key): mem, 'idxs_{}'.format(key): idxs}, \n",
    "                 open(merged_dir, 'wb'))\n",
    "        return mem, idxs, vals\n",
    "            \n",
    "    def merge_results(self, max_samples=None):\n",
    "        \"\"\"Merge all the results from different runs.\n",
    "        \n",
    "        Returns:\n",
    "            combined marginals, sampled indexes and values calculated \n",
    "            using the two algorithms. (If applicable)\n",
    "        \"\"\"\n",
    "        tmc_results = self._merge_parallel_results('tmc', max_samples)\n",
    "        self.marginals_tmc, self.indexes_tmc, self.values_tmc = tmc_results\n",
    "        if self.model_family not in ['logistic', 'NN']:\n",
    "            return\n",
    "        g_results = self._merge_parallel_results('g', max_samples)\n",
    "        self.marginals_g, self.indexes_g, self.values_g = g_results\n",
    "    \n",
    "    def performance_plots(self, vals, name=None, \n",
    "                          num_plot_markers=20, sources=None):\n",
    "        \"\"\"Plots the effect of removing valuable points.\n",
    "        \n",
    "        Args:\n",
    "            vals: A list of different valuations of data points each\n",
    "                 in the format of an array in the same length of the data.\n",
    "            name: Name of the saved plot if not None.\n",
    "            num_plot_markers: number of points in each plot.\n",
    "            sources: If values are for sources of data points rather than\n",
    "                   individual points. In the format of an assignment array\n",
    "                   or dict.\n",
    "                   \n",
    "        Returns:\n",
    "            Plots showing the change in performance as points are removed\n",
    "            from most valuable to least.\n",
    "        \"\"\"\n",
    "        plt.rcParams['figure.figsize'] = 8,8\n",
    "        plt.rcParams['font.size'] = 25\n",
    "        plt.xlabel('Fraction of train data removed (%)')\n",
    "        plt.ylabel('Prediction accuracy (%)', fontsize=20)\n",
    "        if not isinstance(vals, list) and not isinstance(vals, tuple):\n",
    "            vals = [vals]\n",
    "        if sources is None:\n",
    "            sources = {i:np.array([i]) for i in range(len(self.X))}\n",
    "        elif not isinstance(sources, dict):\n",
    "            sources = {i:np.where(sources==i)[0] for i in set(sources)}\n",
    "        vals_sources = [np.array([np.sum(val[sources[i]]) \n",
    "                                  for i in range(len(sources.keys()))])\n",
    "                  for val in vals]\n",
    "        if len(sources.keys()) < num_plot_markers:\n",
    "            num_plot_markers = len(sources.keys()) - 1\n",
    "        plot_points = np.arange(\n",
    "            0, \n",
    "            max(len(sources.keys()) - 10, num_plot_markers),\n",
    "            max(len(sources.keys())//num_plot_markers, 1)\n",
    "        )\n",
    "        perfs = [self._portion_performance(\n",
    "            np.argsort(vals_source)[::-1], plot_points, sources=sources)\n",
    "                 for vals_source in vals_sources]\n",
    "        rnd = np.mean([self._portion_performance(\n",
    "            np.random.permutation(np.argsort(vals_sources[0])[::-1]),\n",
    "            plot_points, sources=sources) for _ in range(10)], 0)\n",
    "        plt.plot(plot_points/len(self.X) * 100, perfs[0] * 100, \n",
    "                 '-', lw=5, ms=10, color='b')\n",
    "        if len(vals)==3:\n",
    "            plt.plot(plot_points/len(self.X) * 100, perfs[1] * 100, \n",
    "                     '--', lw=5, ms=10, color='orange')\n",
    "            legends = ['TMC-Shapley ', 'G-Shapley ', 'LOO', 'Random']\n",
    "        elif len(vals)==2:\n",
    "            legends = ['TMC-Shapley ', 'LOO', 'Random']\n",
    "        else:\n",
    "            legends = ['TMC-Shapley ', 'Random']\n",
    "        plt.plot(plot_points/len(self.X) * 100, perfs[-1] * 100, \n",
    "                 '-.', lw=5, ms=10, color='g')\n",
    "        plt.plot(plot_points/len(self.X) * 100, rnd * 100, \n",
    "                 ':', lw=5, ms=10, color='r')    \n",
    "        plt.legend(legends)\n",
    "        if self.directory is not None and name is not None:\n",
    "            plt.savefig(os.path.join(\n",
    "                self.directory, 'plots', '{}.png'.format(name)),\n",
    "                        bbox_inches = 'tight')\n",
    "            plt.close()\n",
    "            \n",
    "    def _portion_performance(self, idxs, plot_points, sources=None):\n",
    "        \"\"\"Given a set of indexes, starts removing points from \n",
    "        the first elemnt and evaluates the new model after\n",
    "        removing each point.\"\"\"\n",
    "        if sources is None:\n",
    "            sources = {i:np.array([i]) for i in range(len(self.X))}\n",
    "        elif not isinstance(sources, dict):\n",
    "            sources = {i:np.where(sources==i)[0] for i in set(sources)}\n",
    "        scores = []\n",
    "        init_score = self.random_score\n",
    "        for i in range(len(plot_points), 0, -1):\n",
    "            keep_idxs = np.concatenate([sources[idx] for idx \n",
    "                                        in idxs[plot_points[i-1]:]], -1)\n",
    "            X_batch, y_batch = self.X[keep_idxs], self.y[keep_idxs]\n",
    "            if self.sample_weight is not None:\n",
    "                sample_weight_batch = self.sample_weight[keep_idxs]\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "                if (self.is_regression \n",
    "                    or len(set(y_batch)) == len(set(self.y_test))):\n",
    "                    self.restart_model()\n",
    "                    if self.sample_weight is None:\n",
    "                        self.model.fit(X_batch, y_batch)\n",
    "                    else:\n",
    "                        self.model.fit(X_batch, y_batch,\n",
    "                                      sample_weight=sample_weight_batch)\n",
    "                    scores.append(self.value(\n",
    "                        self.model,\n",
    "                        metric=self.metric,\n",
    "                        X=self.X_heldout,\n",
    "                        y=self.y_heldout\n",
    "                    ))\n",
    "                else:\n",
    "                    scores.append(init_score)\n",
    "        return np.array(scores)#[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eaf25fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49a66e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NumNodes: 2708\n",
      "  NumEdges: 10556\n",
      "  NumFeats: 1433\n",
      "  NumClasses: 7\n",
      "  NumTrainingSamples: 140\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n"
     ]
    }
   ],
   "source": [
    "dataset=dgl.data.CoraGraphDataset()\n",
    "g = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f0586ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn import GraphConv\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GraphConv(in_feats, h_feats)\n",
    "        self.conv2 = GraphConv(h_feats, num_classes)\n",
    "\n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        return h\n",
    "# Create the model with given dimensions\n",
    "model = GCN(g.ndata['feat'].shape[1], 16, dataset.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f1379af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 0, loss: 1.946, val acc: 0.120 (best 0.120), test acc: 0.122 (best 0.122)\n",
      "In epoch 5, loss: 1.894, val acc: 0.386 (best 0.460), test acc: 0.394 (best 0.484)\n",
      "In epoch 10, loss: 1.814, val acc: 0.542 (best 0.542), test acc: 0.562 (best 0.562)\n",
      "In epoch 15, loss: 1.706, val acc: 0.630 (best 0.632), test acc: 0.639 (best 0.641)\n",
      "In epoch 20, loss: 1.575, val acc: 0.668 (best 0.668), test acc: 0.673 (best 0.673)\n",
      "In epoch 25, loss: 1.420, val acc: 0.700 (best 0.700), test acc: 0.713 (best 0.713)\n",
      "In epoch 30, loss: 1.248, val acc: 0.716 (best 0.716), test acc: 0.733 (best 0.733)\n",
      "In epoch 35, loss: 1.069, val acc: 0.732 (best 0.732), test acc: 0.741 (best 0.741)\n",
      "In epoch 40, loss: 0.893, val acc: 0.754 (best 0.754), test acc: 0.753 (best 0.753)\n",
      "In epoch 45, loss: 0.732, val acc: 0.764 (best 0.764), test acc: 0.761 (best 0.761)\n",
      "In epoch 50, loss: 0.591, val acc: 0.772 (best 0.772), test acc: 0.766 (best 0.763)\n",
      "In epoch 55, loss: 0.473, val acc: 0.772 (best 0.772), test acc: 0.767 (best 0.763)\n",
      "In epoch 60, loss: 0.379, val acc: 0.778 (best 0.780), test acc: 0.773 (best 0.770)\n",
      "In epoch 65, loss: 0.304, val acc: 0.772 (best 0.780), test acc: 0.776 (best 0.770)\n",
      "In epoch 70, loss: 0.246, val acc: 0.776 (best 0.780), test acc: 0.782 (best 0.770)\n",
      "In epoch 75, loss: 0.200, val acc: 0.770 (best 0.780), test acc: 0.784 (best 0.770)\n",
      "In epoch 80, loss: 0.165, val acc: 0.774 (best 0.780), test acc: 0.782 (best 0.770)\n",
      "In epoch 85, loss: 0.137, val acc: 0.778 (best 0.780), test acc: 0.780 (best 0.770)\n",
      "In epoch 90, loss: 0.116, val acc: 0.778 (best 0.780), test acc: 0.777 (best 0.770)\n",
      "In epoch 95, loss: 0.099, val acc: 0.774 (best 0.780), test acc: 0.774 (best 0.770)\n"
     ]
    }
   ],
   "source": [
    "def train(g, model):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    best_val_acc = 0\n",
    "    best_test_acc = 0\n",
    "\n",
    "    features = g.ndata['feat']\n",
    "    labels = g.ndata['label']\n",
    "    train_mask = g.ndata['train_mask']\n",
    "    val_mask = g.ndata['val_mask']\n",
    "    test_mask = g.ndata['test_mask']\n",
    "    for e in range(100):\n",
    "        # Forward\n",
    "        logits = model(g, features)\n",
    "\n",
    "        # Compute prediction\n",
    "        pred = logits.argmax(1)\n",
    "\n",
    "        # Compute loss\n",
    "        # Note that you should only compute the losses of the nodes in the training set.\n",
    "        loss = F.cross_entropy(logits[train_mask], labels[train_mask])\n",
    "\n",
    "        # Compute accuracy on training/validation/test\n",
    "        train_acc = (pred[train_mask] == labels[train_mask]).float().mean()\n",
    "        val_acc = (pred[val_mask] == labels[val_mask]).float().mean()\n",
    "        test_acc = (pred[test_mask] == labels[test_mask]).float().mean()\n",
    "\n",
    "        # Save the best validation accuracy and the corresponding test accuracy.\n",
    "        if best_val_acc < val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if e % 5 == 0:\n",
    "            print('In epoch {}, loss: {:.3f}, val acc: {:.3f} (best {:.3f}), test acc: {:.3f} (best {:.3f})'.format(\n",
    "                e, loss, val_acc, best_val_acc, test_acc, best_test_acc))\n",
    "model = GCN(g.ndata['feat'].shape[1], 16, dataset.num_classes)\n",
    "train(g, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c55b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dshap = DShap(X, y, X_test, y_test, num_test, \n",
    "              sources=None, \n",
    "              sample_weight=None,\n",
    "              model_family=model, \n",
    "              metric='accuracy',\n",
    "              overwrite=True,\n",
    "              directory=directory)\n",
    "dshap.run(100, 0.1, g_run=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a24a77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308adb8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c626e015",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SHAPenv",
   "language": "python",
   "name": "shapenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
